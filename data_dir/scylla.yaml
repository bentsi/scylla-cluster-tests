# Test duration (min). Parameter used to keep instances produced by tests that
# are supposed to run longer than 24 hours from being killed
test_duration: 60
# cassandra-stress command. You can specify everything but the -node parameter,
# which is going to be provided by the test suite infrastructure
stress_cmd: cassandra-stress write cl=QUORUM duration=60m -schema 'replication(factor=3)' -port jmx=6868 -mode cql3 native -rate threads=1000 -pop seq=1..10000000
# tcpdump to try to capture nodetool API
tcpdump: false
# Number list of database nodes in multiple date centers. Legacy type (int): Number of database nodes
n_db_nodes: '6'
# If you want to use more than 1 loader node, I recommend
# increasing the size of the DB instance (instance_type_db parameter),
# since 't2.micro' nodes tend to struggle with a lot of load.
n_loaders: 1
# Nemesis class to use (possible types in sdcm.nemesis). Example: StopStartMonkey
nemesis_class_name: 'ChaosMonkey'
# Nemesis sleep interval to use if None provided specifically
nemesis_interval: 15
# Prefix for your AWS VMs (handy for telling instances from different
# users apart). If you leave this empty, the prefix will be your unix username.
user_prefix: ''
# Failure/post test behavior
# Default: Destroy AWS instances and credentials (destroy)
# Keep AWS instances running and leave credentials alone (keep)
# Stop AWS instances and leave credentials alone (stop)
failure_post_behavior: destroy
# Space node threshold before starting nemesis (bytes)
# The default value is 6GB (6x1024^3 bytes)
# This value is supposed to reproduce
# https://github.com/scylladb/scylla/issues/1140
space_node_threshold: 6442450944
# Type of IP used to connect to machine instances.
# This depends on whether you are running your tests from a machine inside
# your cloud provider, where it makes sense to use 'private', or outside (use 'public')
# Default: Use public IPs to connect to instances (public)
# Use private IPs to connect to instances (private)
ip_ssh_connections: 'public'
# The new db binary will be uploaded to db instance to replace the one
# provided by the ami. This is useful to test out a new scylla version
# without making a new ami.
# update_db_binary: $path_of_the_new_scylla_binary
update_db_binary: ''
# The new db packages will be uploaded to db instance to update the one
# provided by the ami. This is useful to test out a new scylla version
# without making a new ami via rpm package updates.
# update_db_packages: $path_of_folder_with_packages
update_db_packages: ''

es_url:
es_user:
es_password:

# UPGRADE TESTS
# Repo file that is used in upgrade tests( from data_dir). For example:
# scylla.repo.upgrade_1.6, scylla.repo.upgrade_1.7, etc
repo_file: 'scylla.repo.upgrade'
# Major scylla version to update based on 'repo_file'
# For example: 1.7.rc2, 1.7.0, etc...
# By default upgrade till latest major version
new_version: ''
# The new scylla packages will be uploaded to db instance to update the one
# provided by the ami. This is used when we want to upgrade to a version
# that is not yet in the repository.
# upgrade_node_packages: $path_of_folder_with_packages
upgrade_node_packages: ''

backends: !mux
    libvirt: !mux
        cluster_backend: 'libvirt'
        libvirt_uri: 'qemu:///system'
        libvirt_bridge: 'virbr0'
        scylla_repo: 'http://downloads.scylladb.com/rpm/centos/scylla-1.3.repo'
        libvirt_loader_image: '/path/to/your/loader_base_image.qcow2'
        libvirt_loader_image_user: 'centos'
        libvirt_loader_image_password: '123456'
        libvirt_loader_os_type: 'linux'
        libvirt_loader_os_variant: 'centos7.0'
        libvirt_loader_memory: 2048
        libvirt_db_image: '/path/to/your/db_base_image.qcow2'
        libvirt_db_image_user: 'centos'
        libvirt_db_image_password: '123456'
        libvirt_db_os_type: 'linux'
        libvirt_db_os_variant: 'centos7.0'
        libvirt_db_memory: 2048
        libvirt_monitor_image: '/path/to/your/monitor_base_image.qcow2'
        libvirt_monitor_image_user: 'centos'
        libvirt_monitor_image_password: '123456'
        libvirt_monitor_os_type: 'linux'
        libvirt_monitor_os_variant: 'centos7.0'
        libvirt_monitor_memory: 2048
    aws: !mux
        # What is the backend that the suite will use to get machines from.
        cluster_backend: 'aws'
        # From 0.19 on, iotune will require bigger disk, so let's use a big
        # loader instance by default.
        instance_type_loader: 'c3.large'
        # Size of AWS monitor instance
        instance_type_monitor: t2.small
        ami_id_db_scylla_desc: '1.3-dev'
        us_west_1:
            user_credentials_path: ''
            region_name: 'us-west-1'
            security_group_ids: 'sg-dcd785b9'
            subnet_id: 'subnet-10a04c75'
            ami_id_db_scylla: 'ami-19eca679'
            ami_db_scylla_user: 'centos'
            ami_id_loader: 'ami-19eca679'
            ami_loader_user: 'centos'
            ami_id_db_cassandra: 'ami-3cf7c979'
            ami_db_cassandra_user: 'ubuntu'
            ami_id_monitor: 'ami-19eca679'
            ami_monitor_user: 'centos'
        us_west_2:
            user_credentials_path: ''
            region_name: 'us-west-2'
            security_group_ids: 'sg-81703ae4'
            subnet_id: 'subnet-5207ee37'
            ami_id_db_scylla: 'ami-ec3e9e8c'
            ami_db_scylla_user: 'centos'
            ami_id_loader: 'ami-ec3e9e8c'
            ami_loader_user: 'centos'
            ami_id_db_cassandra: 'ami-1cff962c'
            ami_db_cassandra_user: 'ubuntu'
            ami_id_monitor: 'ami-ec3e9e8c'
            ami_monitor_user: 'centos'
        us_east_1:
            user_credentials_path: ''
            region_name: 'us-east-1'
            security_group_ids: 'sg-c5e1f7a0'
            subnet_id: 'subnet-ec4a72c4'
            ami_id_db_scylla: 'ami-79d3f06e'
            ami_db_scylla_user: 'centos'
            ami_id_loader: 'ami-79d3f06e'
            ami_loader_user: 'centos'
            ami_id_db_cassandra: 'ami-ada2b6c4'
            ami_db_cassandra_user: 'ubuntu'
            ami_id_monitor: 'ami-79d3f06e'
            ami_monitor_user: 'centos'
        #multiple datacenters
        us_east_1_and_us_west_2:
            user_credentials_path: ''
            region_name: 'us-east-1 us-west-2'
            security_group_ids: 'sg-c5e1f7a0 sg-81703ae4'
            subnet_id: 'subnet-ec4a72c4 subnet-5207ee37'
            ami_id_db_scylla: 'ami-79d3f06e ami-ec3e9e8c'
            ami_db_scylla_user: 'centos'
            ami_id_loader: 'ami-79d3f06e'
            ami_loader_user: 'centos'
            ami_id_db_cassandra: 'ami-ada2b6c4'
            ami_db_cassandra_user: 'ubuntu'
            ami_id_monitor: 'ami-79d3f06e'
            ami_monitor_user: 'centos'

    openstack: !mux
        cluster_backend: 'openstack'
        openstack_auth_version: '3.x_password'
        openstack_auth_url: 'http://1.2.3.4:5000'
        openstack_user: 'put_your_user_here'
        openstack_password: 'put_your_pass_here'
        openstack_tenant: 'put_your_tenant_here'
        openstack_service_type: 'compute'
        openstack_service_name: 'nova'
        openstack_service_region: 'put_your_region_here'
        openstack_network: 'put_your_network_here'
        # Openstack image: CentOS7
        openstack_image: '56450641-b01e-4357-b7a9-8b02d6286d90'
        openstack_image_username: 'centos'
        openstack_user_credentials: '/path/to/user/credentials.pem'
        openstack_instance_type_db: 'm1.small'
        openstack_instance_type_loader: 'm1.small'
        openstack_instance_type_monitor: 'm1.small'
        scylla_repo: 'http://downloads.scylladb.com/rpm/centos/scylla-1.4.repo'

    gce: !mux
        cluster_backend: 'gce'
        # Put here the path to your user credentials (usually $HOME/.ssh/google_compute_engine). Generated using the gcloud utility
        user_credentials_path: ''
        # The json file is used for the library to auth against the GCE service. Generated using the gcloud utility
        gce_user_credentials: '/path/to/credentials.json'
        # Look for the service account in Compute Engine settings web page
        gce_service_account_email: 'your-service-account@email.com'
        # Look for the project ID in Compute Engine settings web page
        gce_project: 'your-gce-project-id'
        gce_image: 'https://www.googleapis.com/compute/v1/projects/centos-cloud/global/images/family/centos-7'
        # Your e-mail user (if foo@yourcompany.com, foo is the user)
        gce_image_username: 'your-gce-username'
        #
        # gce_root_disk_size_xx units: GB
        # gce_root_disk_type: 'pd-standard' or 'pd-ssd' (far more expensive)
        #
        # DB instance and disk information
        # For large data set tests, 1500 GB of pd-standard yield 180 MB/s, which seems like a good deal
        gce_instance_type_db: 'n1-standard-2'
        gce_root_disk_type_db: 'pd-ssd'
        gce_root_disk_size_db: 50
        gce_n_local_ssd_disk_db: 1
        # Loader instance and disk information
        gce_instance_type_loader: 'n1-highcpu-4'
        gce_root_disk_type_loader: 'pd-standard'
        gce_root_disk_size_loader: 50
        gce_n_local_ssd_disk_loader: 0
        # Monitor instance and disk information
        gce_instance_type_monitor: 'n1-standard-1'
        gce_root_disk_type_monitor: 'pd-standard'
        gce_root_disk_size_monitor: 50
        gce_n_local_ssd_disk_monitor: 0
        scylla_repo: 'http://downloads.scylladb.com/rpm/centos/scylla-1.4.repo'
        us_east_1:
          gce_datacenter: 'us-east1-b'
        # multiple datacenters
        # us_east_1_and_asias_northeast_1:
        #   gce_datacenter: 'us-east1-b asia-northeast1-a'

databases: !mux
    cassandra:
        db_type: cassandra
        instance_type_db: 'm3.large'
        # Spawn a monitor node
        n_monitor_nodes: 1
    scylla:
        db_type: scylla
        # Let's use c3.large since we're using iotune
        # and we'll stress the DB nodes more thoroughly
        instance_type_db: 'c3.large'
        # Spawn a monitor node 
        n_monitor_nodes: 1
